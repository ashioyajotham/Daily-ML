{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, AveragePooling2D, MaxPool2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The task at hand is to detect the presence of asphalt in a set of images in separate folders eg pothole folder contains pothole images and non-pothole folder contains non-pothole images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First step\n",
    "# Load the data\n",
    "# Path: UAPD_ tongzheng_final\\dataset\n",
    "# Considering we don't have train and test dir, we just have different folders of different sets of images we will use ImageDataGenerator to split the data\n",
    "# We will use 80% of the data for training and 20% for validation\n",
    "\n",
    "# ImageDataGenerator is used to generate batches of tensor image data with real-time data augmentation.\n",
    "# The data will be looped over (in batches).\n",
    "# Path: UAPD_ tongzheng_final\\dataset\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    " # rescale: rescaling factor. Defaults to None. If None or 0, no rescaling is applied, otherwise we multiply the data by the value provided (before applying any other transformation).\n",
    " # validation_split: Float. Fraction of images reserved for validation (strictly between 0\n",
    "    # (no validation) and 1 (only validation)). The data will be split into a training set and a validation set based on this parameter.\n",
    "    # Path: UAPD_ tongzheng_final\\dataset\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory=\"dataset\",\n",
    "    target_size=(64, 64),\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=32,\n",
    "    class_mode=\"binary\",\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    directory=\"dataset\",\n",
    "    target_size=(64, 64),\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=32,\n",
    "    class_mode=\"binary\",\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Visualize the data\n",
    "# Path: UAPD_ tongzheng_final\\dataset\n",
    "sample_training_images, _ = next(train_generator)\n",
    "def plotImages(images_arr):\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(20,20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip(images_arr, axes):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plotImages(sample_training_images[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next step: data augmentation\n",
    "# Data augmentation is a strategy that enables practitioners to significantly increase the diversity of data available for training models, without actually collecting new data.\n",
    "# Data augmentation techniques such as cropping, padding, and horizontal flipping are commonly used to train large neural networks.\n",
    "# Data augmentation is used to expand the training dataset in order to improve the performance and ability of the model to generalize.\n",
    "# Path: UAPD_ tongzheng_final\\dataset\n",
    "train_datagen_augmented = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2,\n",
    "    rotation_range=45,\n",
    "    width_shift_range=.15,\n",
    "    height_shift_range=.15,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.5\n",
    ")\n",
    "\n",
    "train_generator_augmented = train_datagen_augmented.flow_from_directory(\n",
    "    directory=\"dataset\",\n",
    "    target_size=(64, 64),\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=32,\n",
    "    class_mode=\"binary\",\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator_augmented = train_datagen_augmented.flow_from_directory(\n",
    "    directory=\"dataset\",\n",
    "    target_size=(64, 64),\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=32,\n",
    "    class_mode=\"binary\",\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Visualize the data\n",
    "# Path: UAPD_ tongzheng_final\\dataset\n",
    "sample_training_images, _ = next(train_generator_augmented)\n",
    "plotImages(sample_training_images[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next step: build the model\n",
    "# Path: UAPD_ tongzheng_final\\dataset\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(64, 64, 3)))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Next step: train the model\n",
    "# Path: UAPD_ tongzheng_final\\dataset\n",
    "history = model.fit(train_generator_augmented, epochs=10, validation_data=validation_generator_augmented)\n",
    "\n",
    "# Next step: evaluate the model\n",
    "# Path: UAPD_ tongzheng_final\\dataset\n",
    "model.evaluate(validation_generator_augmented)\n",
    "\n",
    "# Next step: visualize the model\n",
    "# Path: UAPD_ tongzheng_final\\dataset\n",
    "# Plot the loss\n",
    "plt.plot(history.history['loss'], label='train loss')\n",
    "plt.plot(history.history['val_loss'], label='val loss')\n",
    "plt.legend()\n",
    "plt.show();\n",
    "\n",
    "# Plot the accuracy\n",
    "plt.plot(history.history['accuracy'], label='train acc')\n",
    "plt.plot(history.history['val_accuracy'], label='val acc')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next step: save the model\n",
    "# Path: UAPD_ tongzheng_final\\dataset\n",
    "model.save('asphalt_detection.h5')\n",
    "\n",
    "# Next step: load the model\n",
    "# Path: UAPD_ tongzheng_final\\dataset\n",
    "model = load_model('asphalt_detection.h5')\n",
    "\n",
    "# Next step: test the model\n",
    "# Path: UAPD_ tongzheng_final\\dataset\n",
    "# Test the model\n",
    "img = image.load_img('dataset/asphalt/asphalt_1.jpg', target_size=(64, 64))\n",
    "img = image.img_to_array(img)\n",
    "img = np.expand_dims(img, axis=0)\n",
    "result = model.predict(img)\n",
    "print(result)\n",
    "if result[0][0] == 1:\n",
    "    prediction = 'asphalt'\n",
    "else:\n",
    "    prediction = 'concrete'\n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
