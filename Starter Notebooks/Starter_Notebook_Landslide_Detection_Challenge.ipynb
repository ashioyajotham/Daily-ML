{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nODaxSas0JbR"
      },
      "source": [
        "# Landslide Detection Challenge - Starter Notebook\n",
        "\n",
        "Welcome to the Landslide Detection Challenge! This notebook will guide you through:\n",
        "1. Loading and exploring the multi-band dataset provided in `.npy` format.\n",
        "2. Visualizing the multi-band satellite data and understanding label distribution.\n",
        "3. Building and evaluating a baseline model to classify landslide and non-landslide images.\n",
        "\n",
        "Letâ€™s get started with loading and understanding the data!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejfTrwF10JbV"
      },
      "source": [
        "## Block 1: Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1NuFY_x0JbW"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Input\n",
        "from tensorflow.keras import backend as K\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVQRf-ws0JbX"
      },
      "source": [
        "### Explanation\n",
        "We import the required libraries:\n",
        "- **os**: for file and directory handling.\n",
        "- **numpy**: for numerical operations, particularly for loading `.npy` files.\n",
        "- **pandas**: for data handling with CSV files.\n",
        "- **matplotlib.pyplot**: for visualizing data, such as label distributions.\n",
        "- **sklearn.model_selection.train_test_split**: for splitting data into training and validation sets.\n",
        "- **tensorflow.keras**: for building and training a neural network model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAPsl4Si0JbY"
      },
      "source": [
        "## Block 2: Define Paths and Load CSV Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDyACVkf0JbY",
        "outputId": "b803a45c-fecd-4ac3-bd8d-be6e802ee661"
      },
      "outputs": [],
      "source": [
        "# Define paths for the dataset (remember to unzip the dataset first!)\n",
        "train_csv_path = 'data/Train.csv'  # Path to the training labels CSV file\n",
        "test_csv_path = 'data/Test.csv'    # Path to the test image IDs CSV file\n",
        "train_data_path = 'data/train_data'  # Folder where .npy train files are located\n",
        "test_data_path = 'data/test_data'    # Folder where .npy test files are located\n",
        "\n",
        "# Load Train.csv and inspect the data\n",
        "train_df = pd.read_csv(train_csv_path)\n",
        "print(\"Train.csv:\")\n",
        "print(train_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "WRN5mqzw5wf8",
        "outputId": "b6dd18be-3fee-45a2-9307-b223d809b9fc"
      },
      "outputs": [],
      "source": [
        "train_df.label.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPDktPhp0JbZ"
      },
      "source": [
        "### Explanation\n",
        "- **Define Paths**: Specify paths to `Train.csv`, `Test.csv`, and folders containing `.npy` files for training and testing images.\n",
        "- **Load Train.csv**: We read the `Train.csv` file, which contains `ID` and `label` columns. The `label` is binary, indicating whether the image contains a landslide (1) or not (0).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d5wTihx0Jba"
      },
      "source": [
        "## Block 3: Visualize Label Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "JtaQAjYO0Jba",
        "outputId": "9e0d6669-f6c5-4c4f-a1fb-f374f61f3a44"
      },
      "outputs": [],
      "source": [
        "# Check distribution of labels\n",
        "label_counts = train_df['label'].value_counts()\n",
        "labels = ['No Landslide', 'Landslide']  # Map the labels 0 and 1 to descriptive names\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.bar(labels, label_counts.values, color=['skyblue', 'salmon'])\n",
        "plt.xlabel(\"Class Label\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Distribution of Labels in Training Set\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGrxXiko0Jbb"
      },
      "source": [
        "### Explanation\n",
        "This block visualizes the distribution of labels (landslide vs. non-landslide) in the training data.\n",
        "- `value_counts()` shows the count of each class, and we use a bar chart to display these counts, which helps us understand if the dataset is balanced.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jida8k-a0Jbb"
      },
      "source": [
        "## Block 4: Load, Normalize, and Display Sample Multi-band Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9SgOyTHP0Jbb",
        "outputId": "a165535e-150e-4e3c-e815-e434a3f158b4"
      },
      "outputs": [],
      "source": [
        "# Function to load and normalize .npy images\n",
        "def load_and_normalize_npy_image(image_id, folder_path):\n",
        "    \"\"\"Loads a .npy file, normalizes each band, and returns the normalized image.\"\"\"\n",
        "    image_path = os.path.join(folder_path, f\"{image_id}.npy\")\n",
        "    img = np.load(image_path)\n",
        "\n",
        "    # Normalize each band to the 0-1 range\n",
        "    img_normalized = (img - img.min(axis=(0, 1))) / (img.max(axis=(0, 1)) - img.min(axis=(0, 1)) + 1e-5)\n",
        "    return img_normalized\n",
        "\n",
        "# Band descriptions\n",
        "band_descriptions = [\n",
        "    \"Red\", \"Green\", \"Blue\", \"Near Infrared\",\n",
        "    \"Descending VV (Vertical-Vertical)\", \"Descending VH (Vertical-Horizontal)\",\n",
        "    \"Descending Diff VV\", \"Descending Diff VH\",\n",
        "    \"Ascending VV (Vertical-Vertical)\", \"Ascending VH (Vertical-Horizontal)\",\n",
        "    \"Ascending Diff VV\", \"Ascending Diff VH\"\n",
        "]\n",
        "\n",
        "# Displaying a few example images with all 12 bands\n",
        "example_ids = train_df['ID'].sample(2).values  # Randomly select 2 image IDs for illustration\n",
        "\n",
        "for image_id in example_ids:\n",
        "    img_normalized = load_and_normalize_npy_image(image_id, train_data_path)\n",
        "\n",
        "    # Plot all 12 bands in a 3x4 grid\n",
        "    fig, axes = plt.subplots(3, 4, figsize=(20, 12))  # 3 rows, 4 columns for 12 plots\n",
        "    fig.suptitle(f\"Sample Image ID: {image_id} - All 12 Bands\", fontsize=16)\n",
        "\n",
        "    # Display each of the 12 bands with descriptions\n",
        "    for band in range(12):\n",
        "        row = band // 4  # Calculate row index (0, 1, or 2)\n",
        "        col = band % 4   # Calculate column index (0 to 3)\n",
        "        axes[row, col].imshow(img_normalized[:, :, band], cmap='gray')\n",
        "        axes[row, col].set_title(f\"Band {band + 1}: {band_descriptions[band]}\")\n",
        "        axes[row, col].axis('off')\n",
        "\n",
        "    plt.subplots_adjust(wspace=0.3, hspace=0.4)  # Adjust spacing between plots\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIsE_pDF0Jbc"
      },
      "source": [
        "### Explanation\n",
        "This block provides a complete view of the 12 individual bands with the corrected descriptions for ascending and descending radar bands.\n",
        "\n",
        "1. **Band Descriptions**:\n",
        "   - **Bands 1-4**: Visible and Near Infrared bands (Red, Green, Blue, NIR).\n",
        "   - **Bands 5-8**: Descending radar bands:\n",
        "     - **Band 5**: Descending VV (Vertical-Vertical polarization).\n",
        "     - **Band 6**: Descending VH (Vertical-Horizontal polarization).\n",
        "     - **Band 7**: Descending Diff VV.\n",
        "     - **Band 8**: Descending Diff VH.\n",
        "   - **Bands 9-12**: Ascending radar bands:\n",
        "     - **Band 9**: Ascending VV (Vertical-Vertical polarization).\n",
        "     - **Band 10**: Ascending VH (Vertical-Horizontal polarization).\n",
        "     - **Band 11**: Ascending Diff VV.\n",
        "     - **Band 12**: Ascending Diff VH.\n",
        "\n",
        "2. **Plotting Layout**:\n",
        "   - A 3x4 grid layout displays each band as a grayscale image.\n",
        "   - Each subplot includes the band number and description for easy reference.\n",
        "   - `plt.subplots_adjust` adds spacing between the plots to improve readability.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SP3JYQyC0Jbc"
      },
      "source": [
        "## Block 5: Prepare Data for Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEwCtEyU0Jbc"
      },
      "outputs": [],
      "source": [
        "# BEWARE - THIS TAKES FOREVER TO RUN -- INSTEAD USE A Generator\n",
        "# Avoid loading all images from *.npy into array first.\n",
        "\n",
        "# Path to the folder containing .npy images\n",
        "folder_path = 'data/train_data/'\n",
        "\n",
        "X = np.array([load_and_normalize_npy_image(image_id, folder_path) for image_id in train_df['ID']])\n",
        "y = train_df['label'].values\n",
        "\n",
        "# Perform a stratified split to maintain class distribution\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Define data augmentation for training\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Define a simple generator for validation (no augmentation)\n",
        "val_datagen = ImageDataGenerator()\n",
        "\n",
        "# Define batch size\n",
        "batch_size = 32\n",
        "\n",
        "# Create generators using .flow()\n",
        "train_ds = train_datagen.flow(\n",
        "    X_train, y_train, batch_size=batch_size, seed=42, shuffle=True\n",
        ")\n",
        "\n",
        "val_ds = val_datagen.flow(\n",
        "    X_val, y_val, batch_size=batch_size, seed=42, shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvYIWDAb0Jbc"
      },
      "source": [
        "### Explanation\n",
        "\n",
        "1. **Load Data**:\n",
        "   - We define `load_npy_image` to load `.npy` files as raw images.\n",
        "   - `X` is created by loading each image using `load_npy_image` based on the image IDs in `train_df`.\n",
        "   - `y` contains the labels from `train_df`.\n",
        "\n",
        "2. **Stratified Data Split**:\n",
        "   - We split the data into `X_train`, `X_val`, `y_train`, and `y_val` while preserving class distribution using `stratify=y`.\n",
        "\n",
        "3. **ImageDataGenerator for Training**:\n",
        "   - `train_datagen` is configured with data augmentation options to increase the diversity of the training data.\n",
        "\n",
        "4. **ImageDataGenerator for Validation**:\n",
        "   - `val_datagen` loads the validation data without augmentation.\n",
        "\n",
        "5. **Generators**:\n",
        "   - `train_ds` and `val_ds` are created using `.flow()`, which yields data in batches for efficient training and validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXHdHC4g_IxM",
        "outputId": "948aba6d-0035-471b-92c0-4630592645bc"
      },
      "outputs": [],
      "source": [
        "X_batch, y_batch = train_ds[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLIH7o3LFn7q",
        "outputId": "f1c003cd-cd23-41d6-efec-86749f8178fe"
      },
      "outputs": [],
      "source": [
        "X_batch.shape, y_batch.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_Uk6T9FFtfs",
        "outputId": "caabcee2-d9a1-4981-afc6-486c84651628"
      },
      "outputs": [],
      "source": [
        "y_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXTIuS1lSdIy"
      },
      "outputs": [],
      "source": [
        "X_val_batch, y_val_batch = val_ds[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lp6CWJsvShvc",
        "outputId": "98221cfa-fe8c-4f73-c7a6-dbb74e9c3147"
      },
      "outputs": [],
      "source": [
        "X_val_batch.shape, y_val_batch.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6xK9j0k0Jbc"
      },
      "source": [
        "## Block 6: Define and Compile a CNN Model with Focal Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 961
        },
        "id": "uG2-yhmH0Jbc",
        "outputId": "da9f574e-075f-4d44-fd8b-8b303249d880"
      },
      "outputs": [],
      "source": [
        "# Precision metric\n",
        "def precision_m(y_true, y_pred):\n",
        "    y_true = K.cast(y_true, 'float32')  # Cast y_true to float32 to match y_pred type\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "# Recall metric\n",
        "def recall_m(y_true, y_pred):\n",
        "    y_true = K.cast(y_true, 'float32')  # Cast y_true to float32 to match y_pred type\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "\n",
        "# F1 Score metric\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
        "\n",
        "\n",
        "# Define the Focal Loss function\n",
        "def focal_loss(gamma=2.0, alpha=0.25):\n",
        "    \"\"\"\n",
        "    Focal Loss for binary classification.\n",
        "\n",
        "    Parameters:\n",
        "        gamma (float): Focusing parameter; typically set to 2.0.\n",
        "        alpha (float): Balancing factor; typically set to 0.25.\n",
        "\n",
        "    Returns:\n",
        "        Binary Focal Loss function.\n",
        "    \"\"\"\n",
        "    def focal_loss_fixed(y_true, y_pred):\n",
        "        # Clip predictions to prevent log(0)\n",
        "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
        "\n",
        "        # Calculate p_t\n",
        "        p_t = tf.where(K.equal(y_true, 1), y_pred, 1 - y_pred)\n",
        "\n",
        "        # Calculate focal loss\n",
        "        fl = -alpha * K.pow(1 - p_t, gamma) * K.log(p_t)\n",
        "        return K.mean(fl)\n",
        "\n",
        "    return focal_loss_fixed\n",
        "\n",
        "# Define the CNN model\n",
        "model = Sequential([\n",
        "    # First convolutional block\n",
        "    Input(shape=X_batch.shape[1:]),\n",
        "    Conv2D(32, (3, 3), activation='relu', data_format='channels_last'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Dropout(0.25),\n",
        "\n",
        "    # Second convolutional block\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Dropout(0.25),\n",
        "\n",
        "    # Third convolutional block\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Dropout(0.25),\n",
        "\n",
        "    # Fourth convolutional block for deeper feature extraction\n",
        "    Conv2D(256, (3, 3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Dropout(0.5),\n",
        "\n",
        "    # Flatten and add dense layers\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),  # Dropout for regularization\n",
        "    Dense(64, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dense(1, activation='sigmoid')  # Output layer with sigmoid activation for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model with Focal Loss and additional metrics\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=focal_loss(gamma=2.0, alpha=0.5),\n",
        "    metrics=['accuracy', precision_m, recall_m, f1_m]  # Additional metrics\n",
        ")\n",
        "\n",
        "# Display the model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rg__4FUJ0Jbd"
      },
      "source": [
        "### Explanation\n",
        "\n",
        "This code defines a Convolutional Neural Network (CNN) with custom metrics (Precision, Recall, and F1 Score) and **Focal Loss** for training, making it suitable for imbalanced datasets.\n",
        "\n",
        "#### Key Components\n",
        "\n",
        "1. **Custom Metrics**:\n",
        "   - `precision_m`: Calculates the proportion of true positive predictions out of all positive predictions, which helps evaluate the modelâ€™s accuracy in predicting positive (landslide) samples.\n",
        "   - `recall_m`: Measures the proportion of true positives out of all actual positives, reflecting the modelâ€™s ability to detect all positive cases.\n",
        "   - `f1_m`: Combines Precision and Recall into a single score using the harmonic mean, making it useful for evaluating the model on imbalanced datasets.\n",
        "\n",
        "2. **Focal Loss Function**:\n",
        "   - Focal Loss is designed to focus on hard-to-classify examples, making it particularly beneficial for imbalanced datasets.\n",
        "   - Parameters:\n",
        "     - `gamma=2.0`: Adjusts the focusing mechanism. Higher values place more focus on misclassified examples.\n",
        "     - `alpha=0.25`: Balances the contribution of positive and negative samples, ensuring the loss calculation doesnâ€™t get dominated by the majority class.\n",
        "   - The function `focal_loss_fixed` calculates Focal Loss by:\n",
        "     - Clipping predictions to avoid `log(0)`.\n",
        "     - Calculating the probability for each prediction (`p_t`), where correct predictions contribute less to the loss.\n",
        "     - Applying the focal scaling factor `(1 - p_t)^{\\gamma}` to emphasize harder examples in the loss computation.\n",
        "\n",
        "3. **CNN Model Architecture**:\n",
        "   - The CNN is designed with four convolutional blocks, each containing:\n",
        "     - **Conv2D layers**: Extract features with increasing complexity as the model goes deeper.\n",
        "     - **BatchNormalization layers**: Normalize activations, speeding up convergence and improving stability.\n",
        "     - **MaxPooling2D layers**: Down-sample feature maps, reducing spatial dimensions and capturing abstract patterns.\n",
        "     - **Dropout layers**: Applied with increasing rates, reducing overfitting by randomly deactivating nodes during training.\n",
        "   - Following the convolutional blocks:\n",
        "     - **Flatten**: Converts 2D feature maps to a 1D vector.\n",
        "     - **Dense layers**: Two fully connected layers with ReLU activation capture higher-level features, with Dropout for regularization.\n",
        "     - **Sigmoid Output Layer**: Used for binary classification, outputs the probability of each class (No Landslide or Landslide).\n",
        "\n",
        "4. **Model Compilation**:\n",
        "   - `optimizer='adam'`: An adaptive optimizer that adjusts the learning rate automatically during training.\n",
        "   - `loss=focal_loss(gamma=2.0, alpha=0.25)`: Focal Loss to handle class imbalance.\n",
        "   - `metrics=['accuracy', precision_m, recall_m, f1_m]`: Additional metrics for a comprehensive evaluation of the model's performance on imbalanced data.\n",
        "\n",
        "5. **Model Summary**:\n",
        "   - `model.summary()` displays the modelâ€™s architecture, showing layer types, output shapes, and parameter counts. This summary helps verify that the model structure matches expectations before training begins."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ukG0Zjk0Jbd"
      },
      "source": [
        "## Block 7: Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "qL32Y2Gi0Jbd",
        "outputId": "e609cea7-aedd-4745-cc4b-f77b63690c41"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Create a checkpoint callback that saves the best model based on validation loss\n",
        "checkpoint = ModelCheckpoint(\n",
        "    \"best_model.h5\",            # Filepath to save the model\n",
        "    monitor='val_loss',         # Monitor the validation loss\n",
        "    verbose=1,                  # Verbosity mode; 1 prints messages when a new best is found\n",
        "    save_best_only=True,        # Save only the model with the best performance\n",
        "    mode='min'                  # 'min' mode because lower validation loss is better\n",
        ")\n",
        "\n",
        "# Train the model using the generators with the checkpoint callback\n",
        "history = model.fit(\n",
        "    train_ds,       # Train generator\n",
        "    epochs=50,\n",
        "    validation_data=val_ds,  # Validation generator\n",
        "    callbacks=[checkpoint]   # Include the checkpoint callback in training\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDA2rhFy0Jbe"
      },
      "source": [
        "### Explanation\n",
        "\n",
        "1. **Model Training**:\n",
        "   - `model.fit` is updated to use the `train_generator` and `val_generator`.\n",
        "   - `steps_per_epoch` and `validation_steps` control how many batches are processed per epoch for training and validation.\n",
        "2. **Efficiency**:\n",
        "   - Using a generator allows the model to load data in batches, reducing memory usage and making training feasible for large datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWs31OAA0Jbe"
      },
      "source": [
        "## Block 8: Plot Training and Validation Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "TVNNaQvJ0Jbe",
        "outputId": "a035e2f5-b66b-4a84-ab7c-8b16071c8628"
      },
      "outputs": [],
      "source": [
        "# Plot training and validation accuracy and loss\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot accuracy\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "# Plot loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Loss')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44qd1L2o0Jbe"
      },
      "source": [
        "### Explanation\n",
        "This plot shows the **training and validation accuracy** as well as the **training and validation loss** over the epochs, allowing us to visually inspect the modelâ€™s learning behavior:\n",
        "\n",
        "- **Steady Improvements in Both Accuracy and Loss**: Consistent increases in accuracy and decreases in loss for both training and validation sets indicate effective learning and good generalization.\n",
        "\n",
        "- **Divergence Between Training and Validation Metrics**:\n",
        "  - If **training accuracy is high** but **validation accuracy is much lower** (with validation loss increasing), this suggests **overfitting**. The model may perform well on training data but fails to generalize to new data.\n",
        "  - If both **training and validation accuracy remain low** and losses are high, this indicates **underfitting**, meaning the model may not be complex enough to capture patterns in the data.\n",
        "\n",
        "This combined plot of accuracy and loss offers a comprehensive view of model performance, helping us assess both how well the model fits the training data and how well it generalizes to new, unseen data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxJ3bFl30Jbe"
      },
      "source": [
        "## Block 9: Make Predictions on the Test Set and Prepare Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7GNYIMl0Jbe"
      },
      "outputs": [],
      "source": [
        "# Loading the full test dataset is not probably the best approach\n",
        "\n",
        "# Load and predict on the test set\n",
        "test_df = pd.read_csv(test_csv_path)\n",
        "test_ids = test_df['ID'].values\n",
        "X_test = np.array([load_and_normalize_npy_image(image_id, test_data_path) for image_id in test_ids])\n",
        "\n",
        "# Predict probabilities and classify as 0 or 1\n",
        "y_test_pred = (model.predict(X_test) > 0.5).astype(int) # the output is a probability that goes from 0 to 1.\n",
        "                                                        # Here we use 0.5 as the threshold\n",
        "\n",
        "# Count the number of predictions for each class\n",
        "unique, counts = np.unique(y_test_pred, return_counts=True)\n",
        "prediction_counts = dict(zip(unique, counts))\n",
        "print(\"Prediction counts:\", prediction_counts)\n",
        "\n",
        "# Prepare submission file\n",
        "submission_df = pd.DataFrame({\n",
        "    'ID': test_ids,\n",
        "    'label': y_test_pred.flatten()  # Flatten to match submission format\n",
        "})\n",
        "submission_df.to_csv('Submission_File.csv', index=False)\n",
        "print(\"Sample submission file created as 'Submission_File.csv'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeyImf6L0Jbf"
      },
      "source": [
        "### Explanation\n",
        "1. **Count Predictions**:\n",
        "   - After making predictions on `X_test`, we use `np.unique` with `return_counts=True` to count the occurrences of `0`s and `1`s in `y_test_pred`.\n",
        "   - We print the counts, which shows the distribution of predicted labels.\n",
        "\n",
        "2. **Interpretation**:\n",
        "   - The counts provide insight into whether the model is predicting a balanced number of `0`s and `1`s or if it's skewed towards one class.\n",
        "   - Please consider that the test set is imbalanced towards the non-landslide class.\n",
        "   - This check is particularly useful for binary classification problems where class imbalance could impact the modelâ€™s evaluation.\n",
        "\n",
        "3. **Prepare Submission File**:\n",
        "   - The `Submission_File.csv` file is created in the same way, ready for submission."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tf",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
