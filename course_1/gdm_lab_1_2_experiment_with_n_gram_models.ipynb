{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashioyajotham/Daily-ML/blob/main/course_1/gdm_lab_1_2_experiment_with_n_gram_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "_DAsej7XSlYt"
      },
      "cell_type": "markdown",
      "source": [
        "> <p><small><small>This Notebook is made available subject to the licence and terms set out in the <a href = \"http://www.github.com/google-deepmind/ai-foundations\">AI Research Foundations Github README file</a>."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://storage.googleapis.com/dm-educational/assets/ai_foundations/GDM-Labs-banner-image-C1-white-bg.png\">"
      ],
      "metadata": {
        "id": "3myzQnLMOJ91"
      }
    },
    {
      "metadata": {
        "id": "zE0jaJsaICiX"
      },
      "cell_type": "markdown",
      "source": [
        "# Lab: Experiment with N-Gram Models\n",
        "\n",
        "<a href='https://colab.research.google.com/github/google-deepmind/ai-foundations/blob/master/course_1/gdm_lab_1_2_experiment_with_n_gram_models.ipynb' target='_parent'><img src='https://colab.research.google.com/assets/colab-badge.svg' alt='Open In Colab'/></a>\n",
        "\n",
        "Practice extracting n-gram counts and learn how to use them to build a language model.\n",
        "\n",
        "\n",
        "30 minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "In the previous lab, you built a very small language model in which you manually assigned probabilities to the next token for different prompts. In this lab, you will automatically estimate the probabilities for predicting the next word and build an **n-gram model** using a small dataset of paragraphs. This will result in a language model that will be able to predict the next word for a given prompt and that can be used to generate texts. You will also gain a practical understanding of how n-gram models capture language patterns and what the limitations of this family of models are. This knowledge will serve as a foundation for exploring more advanced language models in later modules."
      ],
      "metadata": {
        "id": "zSYq3aNrdpnC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What you will learn:\n",
        "\n",
        "By the end of this lab, you will be able to:\n",
        "\n",
        "* Split paragraphs in a dataset into word-like units called tokens, a process known as **tokenization**.\n",
        "* Estimate the probabilities for an n-gram language model from a dataset.\n",
        "* Use the n-gram language model to predict individual tokens and longer continuations.\n",
        "\n"
      ],
      "metadata": {
        "id": "v4Qf3eGhfl4H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tasks\n",
        "\n",
        "As mentioned in the previous article, an n-gram is a continuous sequence of $n$ words. An n-gram model uses these sequences to estimate the probability of the next word given a preceding sequence of $n-1$ words (the context).\n",
        "\n",
        "Recall that you can compute the probability $P(\\mbox{B} \\mid \\mbox{A})$, where $\\mbox{B}$ is the next word and $\\mbox{A}$ is the context, as follows:\n",
        "\n",
        "$$P(\\mbox{B} \\mid \\mbox{A}) = \\frac{\\mbox{Count}(\\mbox{A B})}{\\mbox{Count}(\\mbox{A})}$$\n",
        "\n",
        "The full n-gram counts, $\\mbox{ Count}(\\mbox{A B})$, and the context n-gram counts, $\\mbox{ Count}(\\mbox{A})$, can be computed by counting n-grams in a **dataset**. For building a language model, this dataset is usually a collection of texts, also referred to as a **text corpus**.\n",
        "\n",
        "**In this lab, you will**:\n",
        "\n",
        "* Define your dataset, and break the sentences into individual tokens.\n",
        "* Create n-grams from the tokenized tokens, and calculate counts of n-grams, $\\mbox{ Count}(\\mbox{A B})$.\n",
        "* Estimate $P(\\mbox{B} \\mid \\mbox{A})$ using the n-gram counts.\n",
        "* Use the estimated $P(\\mbox{B} \\mid \\mbox{A})$ distributions to generate new text based on your n-gram  language model.\n",
        "\n"
      ],
      "metadata": {
        "id": "qIhcVxx0foVo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to use Google Colaboratory (Colab)"
      ],
      "metadata": {
        "id": "NDWsJUGcf4Ru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google Colaboratory (also known as Google Colab) is a platform that allows you to run Python code in your browser. The code is written in *cells* that are executed on a remote server.\n",
        "\n",
        "To run a cell, hover over a cell and click on the `run` button to its left. The run button is the circle with the triangle (▶). Alternatively, you can also click on a cell and use the keyboard combination Ctrl+Return (or ⌘+Return if you are using a Mac).\n",
        "\n",
        "To try this out, run the following cell. This should print today's day of the week below it."
      ],
      "metadata": {
        "id": "wlNG_jg-39Zj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "print(f\"Today is {datetime.today():%A}.\")"
      ],
      "metadata": {
        "id": "UyTT6C0JhGBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the *order in which you run the cells matters*. When you are working through a lab, make sure to always run *all* cells in order, otherwise the code might not work. If you take a break while working on a lab, Colab may disconnect you and in that case, you have to execute all cells again before  continuing your work. To make this easier, you can select the cell you are currently working on and then choose **Runtime** → **Run before**  from the menu above (or use the keyboard combination Ctrl/⌘ + F8). This will re-execute all cells before the current one."
      ],
      "metadata": {
        "id": "pbtgZxrpjm6j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports\n",
        "\n",
        "The code in this lab uses the [`random`](https://docs.python.org/3/library/random.html) package for sampling from probability distributions, the [`Counter`](https://docs.python.org/3/library/collections.html#collections.Counter) and [`defaultdict`](https://docs.python.org/3/library/collections.html#collections.defaultdict) data types for counting n-grams, and the [`pandas`](https://pandas.pydata.org/) (`pd`) package for constructing data tables."
      ],
      "metadata": {
        "id": "WQQlDe0hL8AY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install \"git+https://github.com/google-deepmind/ai-foundations.git@main\"\n",
        "\n",
        "# Packages used.\n",
        "import random # For sampling from probability distributions.\n",
        "from collections import Counter, defaultdict # For counting n-grams.\n",
        "\n",
        "import textwrap # For automatically addding linebreaks to long texts.\n",
        "import pandas as pd # For construction and visualizing tables.\n",
        "\n",
        "# Custom functions for providing feedback on your solutions.\n",
        "from ai_foundations.feedback.course_1 import ngrams"
      ],
      "metadata": {
        "id": "5Ht3mC6LUcUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1rTj9QNCr2NC"
      },
      "cell_type": "markdown",
      "source": [
        "## Dataset loading and tokenization\n",
        "\n",
        "Begin by loading the dataset that you will use to estimate the n-gram counts. For this purpose, you will process the  [AfricaGalore](https://storage.googleapis.com/dm-educational/assets/ai_foundations/africa_galore.json) dataset.\n",
        "\n",
        "The Africa Galore dataset has been designed for this course and consists of synthetically generated paragraphs focusing on diverse aspects of African culture, history, and geography. It has been generated using Google's Gemini language model. Because it is synthetically created, the data is clean, free from the noise and inconsistencies that are often present in real-world datasets. At the same time, given its synthetic nature, the texts may not always be as natural as human-authored texts.\n",
        "\n",
        "The dataset is specifically designed for educational purposes and the generation process has been guided to ensure that the content is concentrated around the topics relevant to the lab exercises you will be exploring. Its generation process was inspired by the [TinyStories project](https://arxiv.org/abs/2305.07759) [1].\n",
        "\n",
        "Run the following cell to download the dataset."
      ]
    },
    {
      "metadata": {
        "id": "Pa22xutOAiTx"
      },
      "cell_type": "code",
      "source": [
        "africa_galore = pd.read_json(\n",
        "    \"https://storage.googleapis.com/dm-educational/assets/ai_foundations/africa_galore.json\"\n",
        ")\n",
        "dataset = africa_galore[\"description\"]\n",
        "print(f\"The dataset consists of {dataset.shape[0]} paragraphs.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "xUt0FYhXKg_N"
      },
      "cell_type": "markdown",
      "source": [
        "To get a sense of what these paragraphs look like, inspect the first ten paragraphs in the dataset. You may also want to skim the remainder of the dataset here: [Africa Galore dataset](https://storage.googleapis.com/dm-educational/assets/ai_foundations/africa_galore.json). When working with datasets, it is important to have a good sense of what is in the dataset, since datasets strongly influence the behavior of machine learning models."
      ]
    },
    {
      "metadata": {
        "id": "IRYzvOkxCILc"
      },
      "cell_type": "code",
      "source": [
        "for paragraph in dataset[:10]:\n",
        "    # textwrap automatically adds linebreaks to make long texts more readable.\n",
        "    formatted_paragraph = textwrap.fill(paragraph)\n",
        "    print(f\"{formatted_paragraph}\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "q-wtFwrL5_gn"
      },
      "cell_type": "markdown",
      "source": [
        "### Tokenization\n",
        "\n",
        "Remember that an n-gram is a sequence of $n$ *words*. However, in its current form, the paragraphs are one long string. In order to split the dataset into n-grams and to count them so that you can use them to build a language model, you will have to split these sequences into individual words. This process is referred to as **tokenization**.\n",
        "\n",
        "The simplest tokenizer is a **space tokenizer**. This tokenizer breaks sentences into individual words based on spaces, that is, the characters that are produced by pressing the space bar. For example, a space tokenizer would tokenize the sentence \"Bimpe didn't buy the rice\" into the list of words `[\"Bimpe\", \"didn't\", \"buy\", \"the\", \"rice\"]`.\n",
        "\n",
        "The cell below implements a space tokenizer in the `space_tokenize` function."
      ]
    },
    {
      "metadata": {
        "id": "mns1I5yzx-kp"
      },
      "cell_type": "code",
      "source": [
        "def space_tokenize(text: str) -> list[str]:\n",
        "    \"\"\"Splits a string into a list of words (tokens).\n",
        "\n",
        "    Splits text on space.\n",
        "\n",
        "    Args:\n",
        "        text: The input text.\n",
        "\n",
        "    Returns:\n",
        "        A list of tokens. Returns empty list if text is empty or all spaces.\n",
        "    \"\"\"\n",
        "    tokens = text.split(\" \")\n",
        "    return tokens\n",
        "\n",
        "# Tokenize an example text with the `space_tokenize` function.\n",
        "space_tokenize(\"Kanga, a colorful printed cloth is more than just a fabric.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "j_9Ns7sUB-VT"
      },
      "cell_type": "markdown",
      "source": [
        "Note that the space tokenizer is quite naive. When you are tokenizing based only on spaces, you will observe that the punctuation marks are often considered part of the words. For example, tokenizing the sentence \"Table mountain is tall.\"  will result in a different list of words than tokenizing \"Table mountain is tall\". The first sentence results in (`[\"Table\", \"mountain\", \"is\", \"tall.\"]`. The second sentence, which has the period missing from the end, will result in `[\"Table\", \"mountain\", \"is\", \"tall\"]`). Since the units that tokenizers split sequences into are not always a word, they are usually referred to as **tokens**. In many cases, a token will be the same as a word but it may also be non-word strings, such as \"tall.\" or \"3/4\".\n",
        "\n",
        "You will learn about more sophisticated methods for tokenizing texts in later courses. For the purpose of this lab, the space tokenizer will be sufficient and you will use the implementation of the `space_tokenize` function that you observed in the previous cell."
      ]
    },
    {
      "metadata": {
        "id": "ni2mxLCfY-SK"
      },
      "cell_type": "markdown",
      "source": [
        "To get an impression of what the tokenized data looks like, run the cell below to tokenize the first paragraph in the dataset."
      ]
    },
    {
      "metadata": {
        "id": "Xno1DO0sY87o"
      },
      "cell_type": "code",
      "source": [
        "space_tokenize(dataset[0])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "wMiC7Jq0YtXq"
      },
      "cell_type": "markdown",
      "source": [
        "## Coding Activity 1: From lists of tokens to n-grams\n",
        "\n",
        "The `space_tokenize` function returns a list of individual tokens. However, to compute the conditional probability of a token $\\mbox{B}$ following a context $\\mbox{A}$, $P(\\mbox{B} \\mid \\mbox{A})$, you need to determine how often all n-grams and (n-1)-grams appear in your dataset.\n",
        "\n",
        "For example, if you want to compute the probability of the token \"is\" following a bigram (2-gram) \"Table Mountain\", then you need to know the counts of the trigram (3-gram) \"Table Mountain is\" and the bigram \"Table Mountain\". More generally, to build an n-gram language model, you need to determine the counts of all n-grams and (n-1)-grams. As a first step towards obtaining these counts, you will write a function that turns a list of tokens into a list of n-grams."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "------\n",
        "> 💻 **Your task**:\n",
        ">\n",
        "> Complete the function `generate_ngrams(text: str, n: int)` below.\n",
        ">\n",
        "> This function should return a list of n-grams of length $n$ for a text. Each n-gram should be represented as a tuple of tokens. The function should therefore return a list of tuples of strings. You can use the [`tuple()`](https://www.w3schools.com/python/python_tuples.asp) function to convert a list of strings to a tuple of strings.\n",
        ">\n",
        "> Your function will first have to tokenize the text. You can use the `space_tokenize` function from above for this purpose. Second, the function needs to construct the list of n-grams for the text.\n",
        ">\n",
        "> For example, if the input to the function is `text = \"Table Mountain is tall.\"` and `n = 2`, the function should return the following list of bigrams:\n",
        "> ```\n",
        "> [\n",
        "  (\"Table\", \"Mountain\"),\n",
        "  (\"Mountain\", \"is\"),\n",
        "  (\"is\", \"tall.\")\n",
        "]\n",
        "> ```\n",
        ">\n",
        "> Once you have finished your implementation, run the cell below to print the first ten unigrams, bigrams, and trigrams that appear in the dataset.\n",
        "\n",
        "------"
      ],
      "metadata": {
        "id": "M9qHlL3l8yMx"
      }
    },
    {
      "metadata": {
        "id": "IJx9P17ox-kp"
      },
      "cell_type": "code",
      "source": [
        "all_unigrams = []\n",
        "all_bigrams = []\n",
        "all_trigrams = []\n",
        "\n",
        "def generate_ngrams(text: str, n: int) -> list[tuple[str]]:\n",
        "    \"\"\"Generates n-grams from a given text.\n",
        "\n",
        "    Args:\n",
        "        text: The input text string.\n",
        "        n: The size of the n-grams (e.g., 2 for bigrams, 3 for trigrams).\n",
        "\n",
        "    Returns:\n",
        "        A list of n-grams, each represented as a list of tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenize text.\n",
        "    tokens = ...  # Add your code here.\n",
        "\n",
        "    # Construct the list of n-grams.\n",
        "    ngrams = []\n",
        "\n",
        "    # Add your code here.\n",
        "\n",
        "    return ngrams\n",
        "\n",
        "for paragraph in dataset:\n",
        "    # Calling `generate_ngrams` with n=1 constructs a list of unigrams.\n",
        "    all_unigrams.extend(generate_ngrams(paragraph, n=1))\n",
        "    # Calling `generate_ngrams` with n=2 constructs a list of bigrams (2-grams).\n",
        "    all_bigrams.extend(generate_ngrams(paragraph, n=2))\n",
        "    # Calling `generate_ngrams` with n=2 constructs a list of trigram (3-grams).\n",
        "    all_trigrams.extend(generate_ngrams(paragraph, n=3))\n",
        "\n",
        "print(\"First 10 Unigrams:\", all_unigrams[:10])\n",
        "print(\"First 10 Bigrams:\", all_bigrams[:10])\n",
        "print(\"First 10 Trigrams:\", all_trigrams[:10])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run this cell to test your implementation.\n",
        "ngrams.test_generate_ngrams(generate_ngrams, space_tokenize)"
      ],
      "metadata": {
        "id": "tlT3xhwcsEtE",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "esDTLIxaHmLA"
      },
      "cell_type": "markdown",
      "source": [
        "The main reason for counting n-grams and using these counts to compute probabilities is that these probabilities can capture **patterns** in language. In this context, frequent n-grams are usually more interesting than very rare n-grams since they capture frequent co-occurrences of words (e.g., \"Table Mountain\" or \"jollof rice\").\n",
        "\n",
        "Run the following cell to compute which n-grams appear most frequently in the dataset.\n",
        "The output of the cell below is a list of tuples of the format `(ngram, number of occurrences)`. For example, the output shows you that the bigram `(\"is\",  \"a\")` appears 144 times in the Africa Galore dataset."
      ]
    },
    {
      "metadata": {
        "id": "2azMP6I4x-kp"
      },
      "cell_type": "code",
      "source": [
        "# Use the Python Counter data type for computing the counts of all bigrams.\n",
        "# See: https://docs.python.org/3/library/collections.html#collections.Counter\n",
        "bigram_counts = Counter(all_bigrams)\n",
        "\n",
        "# Print the ten most common bigrams.\n",
        "print(\"Most common bigrams:\")\n",
        "for bigram, count in bigram_counts.most_common(10):\n",
        "    print(f\"  ({bigram}, {count})\")\n",
        "\n",
        "# Use the Python Counter data type for computing the counts of all trigrams.\n",
        "trigram_counts = Counter(all_trigrams)\n",
        "\n",
        "# Print the ten most common trigrams.\n",
        "print(\"\\n\\nMost common trigrams:\")\n",
        "for trigram, count in trigram_counts.most_common(10):\n",
        "    print(f\"  ({trigram}, {count})\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Coding Activity 2: Counting n-grams\n",
        "\n",
        "In preparation for computing the probabilities, you require a function that returns the counts of n-grams."
      ],
      "metadata": {
        "id": "GRn18yqADwJD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------\n",
        "> 💻 **Your task**:\n",
        ">\n",
        "> Complete the function `get_ngram_counts(dataset, n)` below.\n",
        ">\n",
        "> This function should return a dictionary of [`Counter`](https://docs.python.org/3/library/collections.html#collections.Counter) objects where the keys are contexts of length n-1 tokens and the values are counters for the last token in the n-gram.\n",
        ">\n",
        ">For example, if the dataset consists of the two sentences \"Table Mountain is tall.\" and \"Table Mountain is beautiful.\" then the function called with `n = 3` should return:\n",
        ">```\n",
        ">{\n",
        ">   \"Table Mountain\": Counter({\"is\": 2}),\n",
        ">   \"Mountain is\": Counter({\"tall\": 1, \"beautiful\": 1})   \n",
        ">}\n",
        ">```\n",
        "------"
      ],
      "metadata": {
        "id": "LDiQN6Lp7lkH"
      }
    },
    {
      "metadata": {
        "id": "VZaL5Lu4x-kp"
      },
      "cell_type": "code",
      "source": [
        "def get_ngram_counts(dataset: list[str], n: int) -> dict[str, Counter]:\n",
        "    \"\"\"Computes the n-gram counts from a dataset.\n",
        "\n",
        "    This function takes a list of text strings (paragraphs or sentences) as\n",
        "    input, constructs n-grams from each text, and creates a dictionary where:\n",
        "\n",
        "    * Keys represent n-1 token long contexts `context`.\n",
        "    * Values are a Counter object `counts` such that `counts[next_token]` is the\n",
        "      count of `next_token` following `context`.\n",
        "\n",
        "    Args:\n",
        "        dataset: The list of text strings in the dataset.\n",
        "        n: The size of the n-grams to generate (e.g., 2 for bigrams, 3 for\n",
        "            trigrams).\n",
        "\n",
        "    Returns:\n",
        "        A dictionary where keys are (n-1)-token contexts and values are Counter\n",
        "        objects storing the counts of each next token for that context.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Define the dictionary as a defaultdict that is automatically initialized\n",
        "    # with an empty Counter object. This allows you to access and set the value\n",
        "    # of ngram_counts[context][next_token] without initializing\n",
        "    # ngram_counts[context] or ngram_counts[context][next_token] first.\n",
        "    # Reference\n",
        "    # https://docs.python.org/3/library/collections.html#collections.Counter and\n",
        "    # https://docs.python.org/3/library/collections.html#collections.defaultdict\n",
        "    # for more information on how to use defaultdict and Counter types.\n",
        "    ngram_counts = defaultdict(Counter)\n",
        "\n",
        "    for paragraph in dataset:\n",
        "        # Add your code here.\n",
        "        ...\n",
        "\n",
        "    return dict(ngram_counts)\n",
        "\n",
        "\n",
        "# Example usage of the function.\n",
        "example_data = [\n",
        "    \"This is an example sentence.\",\n",
        "    \"Another example sentence.\",\n",
        "    \"Split a sentence.\"\n",
        "]\n",
        "ngram_counts = get_ngram_counts(example_data, 2)\n",
        "\n",
        "# Print the bigram counts dictionary for the dataset consisting of the\n",
        "# three example sentences.\n",
        "print(\"Bigram counts dictionary:\\n\")\n",
        "print(\"{\")\n",
        "for context, counter in ngram_counts.items():\n",
        "    print(f\"  '{context}': {counter},\")\n",
        "print(\"}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "MjHpBNaNknJQ"
      },
      "cell_type": "markdown",
      "source": [
        "As you can see in the output, the count of the bigram \"example sentence.\" is 2, which is shown in the entry for `\"example\"`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run this cell to test your implementation.\n",
        "ngrams.test_ngram_counts(get_ngram_counts, generate_ngrams)"
      ],
      "metadata": {
        "id": "detz3eAXF7AY",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yOC1Lkbb_QuO"
      },
      "cell_type": "markdown",
      "source": [
        "### Exploring the n-gram counts in the Africa Galore dataset\n",
        "\n",
        "You can now use the `get_ngram_counts` function to compute the bigram counts for all combinations of tokens in the Africa Galore dataset.\n",
        "\n",
        "Run the following cell to print a table of bigram counts. This table shows the count of all bigrams where the first token in the bigram is shown at the beginning of each row and the second token is shown at top of each column."
      ]
    },
    {
      "metadata": {
        "id": "qRHDd6pTkBY8"
      },
      "cell_type": "code",
      "source": [
        "bigram_counts = get_ngram_counts(dataset, n=2)\n",
        "\n",
        "# Use the pandas library to display the counts in a table.\n",
        "bigram_counts_matrix = {\n",
        "    context: dict(counts) for context, counts in bigram_counts.items()\n",
        "}\n",
        "bigram_data_frame = pd.DataFrame.from_dict(\n",
        "    bigram_counts_matrix, orient=\"index\").fillna(0)\n",
        "\n",
        "display(bigram_data_frame)\n",
        "\n",
        "zero_count = (bigram_data_frame == 0).sum().sum()\n",
        "print(\n",
        "    f\"Number of bigrams with a count of 0: {zero_count:,}\"\n",
        "    f\" ({zero_count/bigram_data_frame.size * 100:.2f}%)\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "rqbmtcWNmdbK"
      },
      "cell_type": "markdown",
      "source": [
        "As you can observe from the table, a lot of entries are 0. This is because the table shows (almost) every possible combination of all tokens that appear in the Africa Galore dataset. In total, there are $5,143 \\times 5,176$ possible combinations but most of them (99.95%) never appear in the dataset. This **sparsity** is an important property to consider when building n-gram language models. For any context $\\mbox{A}$, the probability $P(\\mbox{B} \\mid \\mbox{A})$ will be 0 for most tokens $\\mbox{B}$.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sparsity increases even more as the length of the context increases. For an example of this, run the following cell, which computes the frequencies of all trigrams and displays them as a table:"
      ],
      "metadata": {
        "id": "ZfgvmlhkBj-F"
      }
    },
    {
      "metadata": {
        "id": "blX_r7XXlJON"
      },
      "cell_type": "code",
      "source": [
        "trigram_counts = get_ngram_counts(dataset, n=3)\n",
        "\n",
        "# Use the pandas library to display the counts in a table.\n",
        "trigram_counts_matrix = {\n",
        "    context: dict(counts) for context, counts in trigram_counts.items()\n",
        "}\n",
        "trigram_data_frame = pd.DataFrame.from_dict(\n",
        "    trigram_counts_matrix, orient=\"index\").fillna(0)\n",
        "\n",
        "display(trigram_data_frame)\n",
        "\n",
        "zero_count = (trigram_data_frame == 0).sum().sum()\n",
        "print(\n",
        "    f\"Number of trigrams with a count of 0: {zero_count:,}\"\n",
        "    f\" ({zero_count/trigram_data_frame.size * 100:.2f}%)\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The table above contains even more entries since it contains one entry for each combination of bigram (the columns) and token (the rows) that appears in the dataset. In the case of Africa Galore, this results in $13,411\\times 5,142$ combinations. In this example, an even higher percentage of entries (99.98%) in the table are 0. Keep the concept of sparsity in mind as you use these counts to compute probabilities and use the probabilities to generate texts.\n",
        "\n"
      ],
      "metadata": {
        "id": "9t8JGL9Ql-rY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate P(B | A)\n",
        "\n",
        "In the previous activities, you have laid all the groundwork for performing the last computation: computing the probability of a token $\\mbox{B}$ following a context $\\mbox{A}$.\n",
        "\n",
        "$$P(\\mbox{B} \\mid \\mbox{A}) = \\frac{\\mbox{Count}(\\mbox{A B})}{\\mbox{Count}(\\mbox{A})}$$\n",
        "\n",
        "Using the `get_ngram_counts` function above, you can compute both $\\mbox{Count}(\\mbox{A B})$ and $\\mbox{Count}(\\mbox{A})$. For example, if you wanted to estimate probabilities of a trigram model that uses a context of length 2, you could compute the counts in the numerator and the denominator for a `dataset` as:\n",
        "\n",
        "```python\n",
        "# Counts for the numerator.\n",
        "trigram_counts = get_ngram_counts(dataset, n=3)\n",
        "# Counts for the denominator.\n",
        "bigram_counts = get_ngram_counts(dataset, n=2)\n",
        "```\n",
        "\n",
        "However, there is a small trick that computes the bigram counts directly from the trigram counts without calling `get_ngram_counts` a second time.\n",
        "\n",
        "To observe how this works, consider the trigram counts for all trigrams that start with the bigram \"a staple.\" You can access these using the dictionary `trigram_counts` that is defined above:"
      ],
      "metadata": {
        "id": "eKGfzX-nosk1"
      }
    },
    {
      "metadata": {
        "id": "jy84xgPUlbmq"
      },
      "cell_type": "code",
      "source": [
        "context = \"a staple\"\n",
        "trigram_counts[context]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "luMFOggul7w_"
      },
      "cell_type": "markdown",
      "source": [
        "The counter in the output of the previous cell shows you that the dataset contains the following trigrams starting with \"a staple\":\n",
        "\n",
        "* \"a staple food\" (1 time).\n",
        "* \"a staple in\" (6 times).\n",
        "* \"a staple dish\" (2 times).\n",
        "* \"a staple throughout\" (1 time).\n",
        "* \"a staple of\" (1 time).\n",
        "* \"a staple at\" (1 time).\n",
        "* \"a staple beverage\" (1 time).\n",
        "\n",
        "The trick to get the bigram count of \"a staple\" is to sum the number of trigrams that start with \"a staple.\" From the counter above, we can compute this total by summing $1+6+2+1+1+1+1 = 13$.\n",
        "\n",
        "The following cell shows you how to do this automatically using the `sum()` function and the `values()` method of a counter."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"a staple\"\n",
        "# Compute the bigram count for \"a staple\" with sum().\n",
        "bigram_count_a_staple = sum(trigram_counts[context].values())\n",
        "\n",
        "print(\n",
        "    'Bigram count of \"a staple\" computed indirectly from trigram counts: ',\n",
        "    bigram_count_a_staple,\n",
        ")\n",
        "\n",
        "# Extract the bigram count for \"a staple\" from bigram_counts.\n",
        "print('Bigram count of \"a staple\" computed directly: ',\n",
        "      bigram_counts[\"a\"][\"staple\"])"
      ],
      "metadata": {
        "id": "J3EUTnTBzIDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the output of the cell above shows, both using the `sum()` function and computing the bigram counts indirectly results in the same number."
      ],
      "metadata": {
        "id": "IS9g71QpbeGV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Coding Activity 3: Computing the n-gram probabilities\n",
        "\n",
        "------\n",
        "> 💻 **Your task**:\n",
        ">\n",
        "> Complete the function `build_ngram_model(dataset, n)` below.\n",
        ">\n",
        "> This function should return a dictionary of dictionaries where the keys are contexts of length $n-1$ tokens and the values are a dictionary providing the probabilities of the next token given the context.\n",
        ">\n",
        ">For example, if the dataset consists of the two sentences \"Table Mountain is tall.\" and \"Table Mountain is beautiful.\" then the function called with `n = 3` should return:\n",
        ">```\n",
        ">{\n",
        ">   \"Table Mountain\": {\"is\": 1.0},\n",
        ">   \"Mountain is\": {\"tall\": 0.5, \"beautiful\": 0.5}   \n",
        ">}\n",
        ">```\n",
        "------"
      ],
      "metadata": {
        "id": "UFxUoB5Lb73l"
      }
    },
    {
      "metadata": {
        "id": "isbFiZ2EyyO4"
      },
      "cell_type": "code",
      "source": [
        "def build_ngram_model(\n",
        "    dataset: list[str],\n",
        "    n: int\n",
        ") -> dict[str, dict[str, float]]:\n",
        "    \"\"\"Builds an n-gram language model.\n",
        "\n",
        "    This function takes a list of text strings (paragraphs or sentences) as\n",
        "    input, generates n-grams from each text using the function get_ngram_counts\n",
        "    and converts them into probabilities.  The resulting model is a dictionary,\n",
        "    where keys are (n-1)-token contexts and values are dictionaries mapping\n",
        "    possible next tokens to their conditional probabilities given the context.\n",
        "\n",
        "    Args:\n",
        "        dataset: A list of text strings representing the dataset.\n",
        "        n: The size of the n-grams (e.g., 2 for a bigram model).\n",
        "\n",
        "    Returns:\n",
        "        A dictionary representing the n-gram language model, where keys are\n",
        "        (n-1)-tokens contexts and values are dictionaries mapping possible next\n",
        "        tokens to their conditional probabilities.\n",
        "    \"\"\"\n",
        "\n",
        "    # A dictionary to store P(B | A).\n",
        "    # ngram_model[context][token] should store P(token | context).\n",
        "    ngram_model = {}\n",
        "\n",
        "    # Use the ngram_counts as computed by the get_ngram_counts function.\n",
        "    ngram_counts = get_ngram_counts(dataset, n)\n",
        "\n",
        "    # Loop through the possible contexts. `context` is a string\n",
        "    # and `next_tokens` is a dictionary mapping possible next tokens to their\n",
        "    # counts of following `context`.\n",
        "    for context, next_tokens in ngram_counts.items():\n",
        "\n",
        "        # Compute Count(A) and P(B | A) here.\n",
        "        # Add your code here.\n",
        "        ...\n",
        "\n",
        "    return ngram_model\n",
        "\n",
        "# Test the method above by bulding a simple trigram model.\n",
        "test_dataset = [\"Table Mountain is tall.\", \"Table Mountain is beautiful.\"]\n",
        "test_trigram_model = build_ngram_model(test_dataset, n=3)\n",
        "test_trigram_model"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run this cell to test your implementation.\n",
        "ngrams.test_build_ngram_model(build_ngram_model, get_ngram_counts)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "DMd_rTsod2LR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wm0g-BMtbTgK"
      },
      "cell_type": "markdown",
      "source": [
        "After you have successfully implemented the method above, run the following cell to construct a trigram model that estimates the probabilities from the Africa Galore dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trigram_model = build_ngram_model(dataset, n=3)"
      ],
      "metadata": {
        "id": "2t7N3A3-kluq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To gain an understanding of the patterns that the model learned, inspect a few probability distributions."
      ],
      "metadata": {
        "id": "kI4ci1Qnk0RV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"P(B | \\\"as it\\\") = {trigram_model['as it']}\")\n",
        "\n",
        "print(f\"P(B | \\\"as they\\\") = {trigram_model['as they']}\")"
      ],
      "metadata": {
        "id": "o-rVO6UykzyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fdi1RxLaLvIi"
      },
      "cell_type": "markdown",
      "source": [
        "------\n",
        "> 💭 **Reflection:**\n",
        ">\n",
        "> Do the probabilities that you estimated from the dataset make sense? Do they capture any patterns or rules of English that you know of?\n",
        "------"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a final step in this part of the lab, look at the probability distribution for more contexts. Start with the context \"The name.\""
      ],
      "metadata": {
        "id": "nX-eaZzWpL52"
      }
    },
    {
      "metadata": {
        "id": "8RDyWq24roG2"
      },
      "cell_type": "code",
      "source": [
        "context = \"The name\"\n",
        "trigram_model[context]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "Uli9Gqy9r2un"
      },
      "cell_type": "markdown",
      "source": [
        "Now run the same code with a slightly altered context of \"Their name.\""
      ]
    },
    {
      "metadata": {
        "id": "S6D9iLQNr628"
      },
      "cell_type": "code",
      "source": [
        "context = \"Their name\"\n",
        "trigram_model[context]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "pAchRjq8r8Wb"
      },
      "cell_type": "markdown",
      "source": [
        "As you might observe, when you run the previous cell, this code results in an error. The reason for this is that the bigram \"Their name\" does not exist in the dataset. This means that it is not included in the trigram model that you built from the dataset and the dictionary that stores the probabilities does not contain an entry for this bigram, which is the cause of the `KeyError`.\n",
        "\n",
        "This highlights one limitation of the n-gram language model: for some contexts it cannot generate continuations. While there exist extensions to n-gram models that make them more robust, they are generally limited in their ability to generate continuations for arbitrary contexts.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Coding Activity 4: Using n-gram probabilities to sample next token\n",
        "\n",
        "The purpose of counting n-grams and using them to estimate conditional probability distributions, as you did in the previous activities, was to be able to sample from the distributions to generate new texts. In this activity, you will now explore how you can sample the next token using an n-gram language model."
      ],
      "metadata": {
        "id": "DsYYr09woio1"
      }
    },
    {
      "metadata": {
        "id": "py8WlM2ARNhZ"
      },
      "cell_type": "markdown",
      "source": [
        "As a first step, consider again the code from the previous lab that used the `random.choices` function to sample a token from a list of candidate tokens, repeated in the next cell.\n",
        "\n",
        "Recall that previously, you manually defined the possible next tokens and probabilities. During this activity, you will use the estimated probabilities from the n-gram model to define the possible next tokens and the associated probabilities.\n"
      ]
    },
    {
      "metadata": {
        "id": "YrXfivl8SKyV"
      },
      "cell_type": "markdown",
      "source": [
        "Run the cell below multiple times to see different candidate words being picked:"
      ]
    },
    {
      "metadata": {
        "id": "qNHq3vS1Rj8m"
      },
      "cell_type": "code",
      "source": [
        "# Define a list of tokens.\n",
        "example_candidate_tokens = [\"apple\", \"banana\", \"cherry\"]\n",
        "\n",
        "# Define corresponding probabilities for each fruit.\n",
        "probabilities = [0.2, 0.5, 0.3]\n",
        "\n",
        "# Sample one fruit based on the probabilities.\n",
        "# The 'k=1' parameter instructs the function to return one item.\n",
        "chosen_fruit = random.choices(\n",
        "    example_candidate_tokens,\n",
        "    weights=probabilities,\n",
        "    k=1)[0]\n",
        "\n",
        "print(\"Chosen fruit:\", chosen_fruit)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "CzOxiLBJI_Hv"
      },
      "cell_type": "markdown",
      "source": [
        "------\n",
        "> 💻 **Your task:**\n",
        ">\n",
        "> Complete the following cell and use the probabilities in `trigram_model` to\n",
        "> 1. Generate a list of candidate tokens for the context \"looking for.\"\n",
        "> 2. Extract the corresponding probabilities for each candidate token.\n",
        ">\n",
        "> Run the following cell multiple times to observe what tokens are being sampled.\n",
        "------\n"
      ]
    },
    {
      "metadata": {
        "id": "zBgdyD0XVa3A"
      },
      "cell_type": "code",
      "source": [
        "context = \"looking for\"\n",
        "candidate_tokens = []\n",
        "candidate_tokens_probabilities = []\n",
        "\n",
        "# Extract candidate tokens and associated probabilities from `trigram_model`.\n",
        "# Add your code here.\n",
        "\n",
        "\n",
        "print(f\"Candidate tokens: {candidate_tokens}\")\n",
        "print(f\"Candidate token probabilities: {candidate_tokens_probabilities}\")\n",
        "\n",
        "# Sample from the list of candidate tokens according to the\n",
        "# associated probabilities.\n",
        "next_token = random.choices(candidate_tokens,\n",
        "                            weights=candidate_tokens_probabilities)[0]\n",
        "\n",
        "print(\"\\n\\nSampled next token:\")\n",
        "print(context, next_token)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "ovcAe7nmsDBX",
        "cellView": "form"
      },
      "cell_type": "code",
      "source": [
        "# @title Run this to test your code, or get a hint.\n",
        "ngrams.test_candidate_tokens(\n",
        "    trigram_model, candidate_tokens, candidate_tokens_probabilities\n",
        ")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "oCoGHege65Z1"
      },
      "cell_type": "markdown",
      "source": [
        "## Generating texts\n",
        "\n",
        "You will now investigate the behavior of a function that can generate new texts for a given prompt using the probabilities of an n-gram model.\n",
        "\n",
        "Text generation using an n-gram model is an iterative process where each newly generated token is added to the existing context. This forms the basis for predicting the next token.\n",
        "\n",
        "Starting with an initial prompt text, the model uses the probability distribution derived from the n-gram counts to select the most likely next token. This again makes use of the `random.choices` function for picking the next token. Once this token has been generated, it is added to the context and the updated sequence is used to calculate the next probability distribution. This chain-like process continues until `num_tokens_to_generate` tokens have been generated.\n",
        "\n",
        "The following `generate_next_n_tokens` function implements this iterative generation process:"
      ]
    },
    {
      "metadata": {
        "id": "uenGa3b4iHBo"
      },
      "cell_type": "code",
      "source": [
        "def generate_next_n_tokens(\n",
        "    n: int,\n",
        "    ngram_model: dict[str, dict[str, float]],\n",
        "    prompt: str,\n",
        "    num_tokens_to_generate: int,\n",
        ") -> str:\n",
        "    \"\"\"Generates `num_tokens_to_generate` tokens following a given prompt using\n",
        "    an n-gram language model.\n",
        "\n",
        "    This function takes an n-gram model and uses it to predict the most\n",
        "    likely next token for the given prompt. The generation process\n",
        "    continues iteratively, appending predicted tokens to the prompt until the\n",
        "    desired number of tokens is generated or a context is\n",
        "    encountered for which the model has no predictions.\n",
        "\n",
        "    Args:\n",
        "        n: The size of the n-grams to use (e.g., 2 for a bigram model).\n",
        "        ngram_model: A dictionary representing the n-gram language model.\n",
        "        prompt: The starting text prompt for generating the next tokens.\n",
        "        num_tokens_to_generate: The number of words to generate following\n",
        "            the prompt.\n",
        "\n",
        "    Returns:\n",
        "        A string containing the original prompt followed by the generated\n",
        "        tokens. If no valid continuation is found for a given context, the\n",
        "        function will return the text generated up to that point and print a\n",
        "        message indicating that no continuation could be found.\n",
        "    \"\"\"\n",
        "\n",
        "    # Split prompt into individual tokens.\n",
        "    generated_words = space_tokenize(prompt)\n",
        "\n",
        "    for _ in range(num_tokens_to_generate):\n",
        "        # Get last (n-1) tokens as context.\n",
        "        context = generated_words[-(n - 1):]\n",
        "        context = \" \".join(context)\n",
        "        if context in ngram_model:\n",
        "            # Sample next word based on probabilities.\n",
        "            next_word = random.choices(\n",
        "                list(ngram_model[context].keys()),\n",
        "                weights=ngram_model[context].values()\n",
        "            )[0]\n",
        "\n",
        "            generated_words.append(next_word)\n",
        "        else:\n",
        "            print(\n",
        "                \"⚠️ No valid continuation found. Change the prompt or\"\n",
        "                \" try sampling another continuation.\\n\"\n",
        "            )\n",
        "            break\n",
        "\n",
        "    return \" \".join(generated_words)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "SGJwKbsnIOXy"
      },
      "cell_type": "markdown",
      "source": [
        "### Generating texts with a bigram model\n",
        "\n",
        "First, run the following cell multiple times to generate new continuations using a bigram model whose probabilities were estimated from the Africa Galore dataset.\n"
      ]
    },
    {
      "metadata": {
        "id": "xndDil588L39"
      },
      "cell_type": "code",
      "source": [
        "prompt = \"Jide was hungry so she went looking for\"\n",
        "\n",
        "# Construct a bigram model using the Africa Galore dataset.\n",
        "bigram_model = build_ngram_model(dataset, n=2)\n",
        "\n",
        "n = 2  # Bigram.\n",
        "num_tokens_to_generate = 10  # Generate next n words.\n",
        "generate_next_n_tokens(\n",
        "    n=n,\n",
        "    ngram_model=bigram_model,\n",
        "    prompt=prompt,\n",
        "    num_tokens_to_generate=num_tokens_to_generate,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "AMZCFKMaISrV"
      },
      "cell_type": "markdown",
      "source": [
        "### Generating texts with a trigram model\n",
        "\n",
        "Next, run the following cell multiple times to generate new continuations using the trigram model that you built in the previous activities.\n"
      ]
    },
    {
      "metadata": {
        "id": "0Pq0FW_2IULl"
      },
      "cell_type": "code",
      "source": [
        "prompt = \"Jide was hungry so she went looking for\"\n",
        "\n",
        "n = 3  # Trigram.\n",
        "num_tokens_to_generate = 10  # Generate next n words.\n",
        "generate_next_n_tokens(\n",
        "    n=n,\n",
        "    ngram_model=trigram_model,\n",
        "    prompt=prompt,\n",
        "    num_tokens_to_generate=num_tokens_to_generate,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "Oz0MoqxB96S_"
      },
      "cell_type": "markdown",
      "source": [
        "The different results when running the cell multiple times are because the n-gram model is a stochastic model that samples the next token from a probability distribution. More probable next words have a high probability of getting picked but are not guaranteed to be picked.\n",
        "\n",
        "------\n",
        "> 💭 **Reflection: Comparing the generations of bigram and trigram models**\n",
        ">\n",
        "> As you generate multiple continuations using both a bigram model and a trigram model, take note which continuations make more sense and tend to be grammatically correct. On average, does the bigram model or the trigram model produce more sensible continuations? Which model fails to produce a valid continuation more often?\n",
        "------\n"
      ]
    },
    {
      "metadata": {
        "id": "e5B0zuHWkGZr"
      },
      "cell_type": "markdown",
      "source": [
        "## What happens when you increase the $n$ in n-grams?\n",
        "\n",
        "While it intuitively seems that a larger context (greater $n$) would lead to better quality output by capturing more long-range dependencies in language,  it quickly runs into the problem of data sparsity since most n-grams will never be observed in the dataset.\n",
        "\n",
        "When moving from bigrams (pairs of tokens) to trigrams (triplets of tokens), the number of possible combinations increases exponentially, and many of these triplets rarely, if ever, appear in the dataset. This means that while bigram models can cover a significant portion of common token pairs, the majority of potential token sequences in trigram and higher-order models are underrepresented. This makes it more challenging for the model to reliably predict the next token.\n",
        "\n",
        "Consider a simple vocabulary of five tokens: \"I\", \"love\", \"to\", \"eat\", and \"jollof\". For bigrams, there are at most\n",
        "$$5 \\times 5 = 25 $$\n",
        "possible combinations. In reality, however, your data might only include common pairs like \"I love\", \"love to\", \"to eat\", and \"eat jollof\". Now, when you move to trigrams (triplets of tokens), there are $$ 5 \\times 5 \\times 5 = 125$$ possible combinations. However, only a few of these, such as \"I love to\", \"love to eat\", and \"to eat jollof\", will actually appear in the data.\n",
        "\n",
        "Even with massive datasets, many of the higher-order n-grams will never appear in the corpus. This results in many zero counts for the probabilities. As the n-gram order increases, the number of potential combinations grows exponentially. This often leads to many combinations being rare or absent in the data, which makes reliable estimation of probabilities more difficult.\n"
      ]
    },
    {
      "metadata": {
        "id": "RqOIH4XKt5ny"
      },
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "This is the end of the **Experiment with N-Gram Models** lab.\n",
        "\n",
        "This lab provided a practical exploration of n-gram language models. Here are some key takeaways:\n",
        "\n",
        "**1. Functionality:**\n",
        "\n",
        "- You saw how n-gram models can be used to predict the next token in a sequence based on the preceding tokens (context).\n",
        "- N-gram models are relatively simple to implement by estimating conditional probabilities from n-gram counts in a dataset. These probabilities can then be used to repeatedly sample the next token and generate new continuations.\n",
        "\n",
        "**2. Data sparsity:**\n",
        "\n",
        "- Data sparsity is a major challenge for n-gram models, especially with higher-order n-grams (trigrams or larger).\n",
        "- This sparsity arises because many possible token combinations are rare or absent in real-world text data.\n",
        "- You observed this in the dataset. The dimensions of the trigram matrix are significantly larger than those of the bigram matrix, resulting in more zero values.\n",
        "\n",
        "**3. Randomness and text generation:**\n",
        "\n",
        "- While the model assigns probabilities to different next tokens, the actual choice is stochastic (random), resulting in different outputs for multiple runs.\n",
        "- While higher probabilities increase the chances of a token being picked, less frequent tokens can also be generated.\n",
        "\n",
        "**4. Considerations for text generation:**\n",
        "\n",
        "- The size of *n* can affect the quality of the generated text. Larger *n* might capture longer-range dependencies but can lead to data sparsity and repetitive outputs.\n",
        "- The model is unable to generate text following an n-gram that is not present in the dataset.\n",
        "\n",
        "In the next activity, you will reflect on some of the limitations of n-gram models and go on to compare them with more advanced models."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solutions\n",
        "\n",
        "The following cells provide reference solutions to the coding activities above. If you really get stuck after trying to solve the activities yourself, you may want to consult these solutions.\n",
        "\n",
        "However, we recommend that you *only* look at the solutions after you have tried to solve the activities above *multiple times*. The best way to learn challenging concepts in computer science and artificial intelligence is to debug your code piece-by-piece until it works rather than copying existing solutions.\n",
        "\n",
        "If you feel stuck, you may want to first try to debug your code, for example, by adding additional print statements to see what your code is doing at every step. This will provide you with a much deeper understanding of the code and the materials. It will also provide you with practice on how to solve challenging coding problems beyond this course.\n",
        "\n",
        "To view the solutions for an activity, click on the arrow to the left of the activity name. If you consult the solutions, do not copy and paste them into the cells above. Instead, look at them and then type them manually into the cell. This will help you understand where you went wrong."
      ],
      "metadata": {
        "id": "TbCry1ll6L9g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Coding Activity 1"
      ],
      "metadata": {
        "id": "4NJgcGxO6UDE"
      }
    },
    {
      "metadata": {
        "id": "UD38Eu8K7sXg"
      },
      "cell_type": "code",
      "source": [
        "# This is a complete implementation of `generate_ngrams`.\n",
        "def generate_ngrams(text: str, n: int) -> list[tuple[str]]:\n",
        "    \"\"\"Generates n-grams from a given text.\n",
        "\n",
        "    Args:\n",
        "        text: The input text string.\n",
        "        n: The size of the n-grams (e.g., 2 for bigrams, 3 for trigrams).\n",
        "\n",
        "    Returns:\n",
        "        A list of n-grams, each represented as a list of tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenize text.\n",
        "    tokens = space_tokenize(text)\n",
        "\n",
        "    # Construct the list of n-grams.\n",
        "    ngrams = []\n",
        "\n",
        "    num_of_tokens = len(tokens)\n",
        "\n",
        "    # The last n-gram will be tokens[num_of_tokens - n + 1: num_of_tokens + 1].\n",
        "    for i in range(0, num_of_tokens - n + 1):\n",
        "        ngrams.append(tuple(tokens[i:i+n]))\n",
        "\n",
        "    return ngrams"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Coding Activity 2"
      ],
      "metadata": {
        "id": "AYxczrMh8weO"
      }
    },
    {
      "metadata": {
        "id": "bsdbWiyvAqso"
      },
      "cell_type": "code",
      "source": [
        "# This is a complete implementation of get_ngram_counts.\n",
        "def get_ngram_counts(dataset: list[str], n: int) -> dict[str, Counter]:\n",
        "    \"\"\"Computes the n-gram counts from a dataset.\n",
        "\n",
        "    This function takes a list of text strings (paragraphs or sentences) as\n",
        "    input, constructs n-grams from each text, and creates a dictionary where:\n",
        "\n",
        "    * Keys represent n-1 token long contexts `context`.\n",
        "    * Values are a Counter object `counts` such that `counts[next_token]` is the\n",
        "    * count of `next_token` following `context`.\n",
        "\n",
        "    Args:\n",
        "        dataset: The list of text strings in the dataset.\n",
        "        n: The size of the n-grams to generate (e.g., 2 for bigrams, 3 for\n",
        "            trigrams).\n",
        "\n",
        "    Returns:\n",
        "        A dictionary where keys are (n-1)-token contexts and values are Counter\n",
        "        objects storing the counts of each next token for that context.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Define the dictionary as a defaultdict that is automatically initialized\n",
        "    # with an empty Counter object. This allows you to access and set the value\n",
        "    # of ngram_counts[context][next_token] without initializing\n",
        "    # ngram_counts[context] or ngram_counts[context][next_token] first.\n",
        "    # See\n",
        "    # https://docs.python.org/3/library/collections.html#collections.Counter and\n",
        "    # https://docs.python.org/3/library/collections.html#collections.defaultdict\n",
        "    # for more information on how to use defaultdict and Counter types.\n",
        "    ngram_counts = defaultdict(Counter)\n",
        "\n",
        "    # Loop through all paragraphs.\n",
        "    for paragraph in dataset:\n",
        "        # Loop through all n-grams for the paragraph.\n",
        "        for ngram in generate_ngrams(paragraph, n):\n",
        "            # Extract the context. This will be all but the last token.\n",
        "            context = \" \".join(ngram[:-1])\n",
        "            # Extract the next token. This will be the last token of the n-gram.\n",
        "            next_token = ngram[-1]\n",
        "            # Increment the counter for the context - next_token pair by 1.\n",
        "            ngram_counts[context][next_token] += 1\n",
        "\n",
        "    return dict(ngram_counts)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Coding Activity 3\n"
      ],
      "metadata": {
        "id": "cIYct9V3l2G3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete implemenation of build_ngram_model.\n",
        "def build_ngram_model(dataset: list[str], n: int) -> dict[str, dict[str, float]]:\n",
        "    \"\"\"Builds an n-gram language model.\n",
        "\n",
        "    This function takes a list of text strings (paragraphs or sentences) as\n",
        "    input, generates n-grams from each text using the function get_ngram_counts\n",
        "    and converts them into probabilities.  The resulting model is a dictionary,\n",
        "    where keys are (n-1)-token contexts and values are dictionaries mapping\n",
        "    possible next tokens to their conditional probabilities given the context.\n",
        "\n",
        "    Args:\n",
        "        dataset: A list of text strings representing the dataset.\n",
        "        n: The size of the n-grams (e.g., 2 for a bigram model).\n",
        "\n",
        "    Returns:\n",
        "        A dictionary representing the n-gram language model, where keys are\n",
        "        (n-1)-tokens contexts and values are dictionaries mapping possible next\n",
        "        tokens to their conditional probabilities.\n",
        "    \"\"\"\n",
        "    # A dictionary to store P(B | A).\n",
        "    # ngram_model[context][token] should store P(token | context).\n",
        "    ngram_model = {}\n",
        "\n",
        "    # Use the ngram_counts as computed by the get_ngram_counts function.\n",
        "    ngram_counts = get_ngram_counts(dataset, n)\n",
        "\n",
        "\n",
        "    # Loop through the possible contexts. `context` is a string\n",
        "    # and `next_tokens` is a dictionary mapping possible next tokens to their\n",
        "    # counts of following `context`.\n",
        "    for context, next_tokens in ngram_counts.items():\n",
        "\n",
        "        # Compute Count(A) and P(B | A ) here.\n",
        "        context_total_count = sum(next_tokens.values())\n",
        "        ngram_model[context] = {}\n",
        "        for token, count in next_tokens.items():\n",
        "            ngram_model[context][token] = count / context_total_count\n",
        "\n",
        "    return ngram_model"
      ],
      "metadata": {
        "id": "v9jWTcgSl0vY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Coding Activity 4"
      ],
      "metadata": {
        "id": "Doe3GMRMmquD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Include this code for extracting the candidate tokens and\n",
        "# candidate tokens probabilities.\n",
        "\n",
        "# Extract candidate tokens and associated probabilities from `trigram_model`.\n",
        "for token, prob in trigram_model[context].items():\n",
        "    candidate_tokens.append(token)\n",
        "    candidate_tokens_probabilities.append(prob)\n"
      ],
      "metadata": {
        "id": "3SpVItCZmqI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "[1] Ronen Eldan and Yuanzhi Li. 2023. Tiny Stories: How Small Can Language Models Be and Still Speak Coherent English. arXiv:2305.07759. Retrieved from [https://arxiv.org/pdf/2305.07759](https://arxiv.org/pdf/2305.07759).\n"
      ],
      "metadata": {
        "id": "FwwgTmVkFvte"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "NDWsJUGcf4Ru",
        "TbCry1ll6L9g",
        "4NJgcGxO6UDE",
        "AYxczrMh8weO",
        "cIYct9V3l2G3",
        "Doe3GMRMmquD"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}