{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Initial Exploration\n",
    "Load the customer data and perform initial EDA to understand the available features, missing values, and data distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading and Initial Exploration\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the customer data\n",
    "data = pd.read_csv('customer_data.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "data.head()\n",
    "\n",
    "# Display basic information about the dataset\n",
    "data.info()\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = data.isnull().sum()\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "# Summary statistics of the dataset\n",
    "data.describe()\n",
    "\n",
    "# Plot distributions of numerical features\n",
    "numerical_features = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "data[numerical_features].hist(bins=15, figsize=(15, 10), layout=(5, 3))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot distributions of categorical features\n",
    "categorical_features = data.select_dtypes(include=['object']).columns\n",
    "for feature in categorical_features:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.countplot(data[feature])\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Churn\n",
    "Create a churn definition based on customer behavior patterns and implement it as a binary target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Churn\n",
    "\n",
    "# Create a churn definition based on customer behavior patterns\n",
    "# For this example, let's assume churn is defined as customers who have not made any transactions in the last 6 months\n",
    "\n",
    "# Convert the 'last_transaction_date' to datetime\n",
    "data['last_transaction_date'] = pd.to_datetime(data['last_transaction_date'])\n",
    "\n",
    "# Define the churn threshold date (6 months from the most recent transaction date in the dataset)\n",
    "churn_threshold_date = data['last_transaction_date'].max() - pd.DateOffset(months=6)\n",
    "\n",
    "# Create the churn target variable\n",
    "data['churn'] = (data['last_transaction_date'] < churn_threshold_date).astype(int)\n",
    "\n",
    "# Display the first few rows to verify the churn definition\n",
    "data[['customer_id', 'last_transaction_date', 'churn']].head()\n",
    "\n",
    "# Plot the distribution of the churn variable\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(data['churn'])\n",
    "plt.title('Distribution of Churn')\n",
    "plt.xticks([0, 1], ['Not Churned', 'Churned'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Create relevant features from the raw data, including customer behavior metrics, transaction patterns, and temporal features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "\n",
    "# Create relevant features from the raw data\n",
    "\n",
    "# Feature 1: Total number of transactions\n",
    "data['total_transactions'] = data.groupby('customer_id')['transaction_amount'].transform('count')\n",
    "\n",
    "# Feature 2: Total transaction amount\n",
    "data['total_transaction_amount'] = data.groupby('customer_id')['transaction_amount'].transform('sum')\n",
    "\n",
    "# Feature 3: Average transaction amount\n",
    "data['average_transaction_amount'] = data.groupby('customer_id')['transaction_amount'].transform('mean')\n",
    "\n",
    "# Feature 4: Number of days since last transaction\n",
    "data['days_since_last_transaction'] = (data['last_transaction_date'].max() - data['last_transaction_date']).dt.days\n",
    "\n",
    "# Feature 5: Number of unique transaction types\n",
    "data['unique_transaction_types'] = data.groupby('customer_id')['transaction_type'].transform('nunique')\n",
    "\n",
    "# Feature 6: Number of transactions in the last month\n",
    "last_month_date = data['last_transaction_date'].max() - pd.DateOffset(months=1)\n",
    "data['transactions_last_month'] = data[data['last_transaction_date'] >= last_month_date].groupby('customer_id')['transaction_amount'].transform('count')\n",
    "\n",
    "# Fill NaN values with 0 for transactions_last_month\n",
    "data['transactions_last_month'].fillna(0, inplace=True)\n",
    "\n",
    "# Drop duplicate rows to keep one row per customer\n",
    "data = data.drop_duplicates(subset='customer_id')\n",
    "\n",
    "# Display the first few rows to verify the new features\n",
    "data[['customer_id', 'total_transactions', 'total_transaction_amount', 'average_transaction_amount', 'days_since_last_transaction', 'unique_transaction_types', 'transactions_last_month']].head()\n",
    "\n",
    "# Plot the distribution of the new features\n",
    "new_features = ['total_transactions', 'total_transaction_amount', 'average_transaction_amount', 'days_since_last_transaction', 'unique_transaction_types', 'transactions_last_month']\n",
    "data[new_features].hist(bins=15, figsize=(15, 10), layout=(3, 2))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier Detection\n",
    "Implement unsupervised learning methods to detect outliers that might require separate handling in the prediction system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier Detection\n",
    "\n",
    "# Import necessary libraries for outlier detection\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Select features for outlier detection\n",
    "features_for_outlier_detection = ['total_transactions', 'total_transaction_amount', 'average_transaction_amount', 'days_since_last_transaction', 'unique_transaction_types', 'transactions_last_month']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(data[features_for_outlier_detection])\n",
    "\n",
    "# Initialize the Isolation Forest model\n",
    "iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "\n",
    "# Fit the model and predict outliers\n",
    "outlier_predictions = iso_forest.fit_predict(scaled_features)\n",
    "\n",
    "# Add the outlier predictions to the dataset\n",
    "data['outlier_score'] = outlier_predictions\n",
    "\n",
    "# Plot the distribution of outlier scores\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(data['outlier_score'])\n",
    "plt.title('Distribution of Outlier Scores')\n",
    "plt.xticks([0, 1], ['Inliers', 'Outliers'])\n",
    "plt.show()\n",
    "\n",
    "# Display the first few rows to verify the outlier scores\n",
    "data[['customer_id', 'outlier_score']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training Pipeline\n",
    "Build a training pipeline using a single model architecture (e.g., XGBoost) with proper cross-validation and hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training Pipeline\n",
    "\n",
    "# Import necessary libraries for model training\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Select features and target variable\n",
    "features = ['total_transactions', 'total_transaction_amount', 'average_transaction_amount', 'days_since_last_transaction', 'unique_transaction_types', 'transactions_last_month']\n",
    "target = 'churn'\n",
    "\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the XGBoost classifier\n",
    "xgb_model = XGBClassifier(random_state=42)\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=5, scoring='roc_auc', n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the model to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"ROC AUC Score:\")\n",
    "print(roc_auc_score(y_test, y_pred_proba))\n",
    "\n",
    "# Plot the feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(features, best_model.feature_importances_)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importance of the XGBoost Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule-Based System\n",
    "Develop business rules for clear-cut churn cases that don't require model prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule-Based System\n",
    "\n",
    "# Define business rules for clear-cut churn cases\n",
    "# Rule 1: Customers with no transactions in the last 12 months are considered churned\n",
    "data['rule_no_transactions_12_months'] = (data['days_since_last_transaction'] > 365).astype(int)\n",
    "\n",
    "# Rule 2: Customers with total transaction amount less than $100 in the last 12 months are considered churned\n",
    "data['rule_low_transaction_amount'] = ((data['total_transaction_amount'] < 100) & (data['days_since_last_transaction'] <= 365)).astype(int)\n",
    "\n",
    "# Rule 3: Customers with only one type of transaction in the last 12 months are considered churned\n",
    "data['rule_single_transaction_type'] = ((data['unique_transaction_types'] == 1) & (data['days_since_last_transaction'] <= 365)).astype(int)\n",
    "\n",
    "# Combine the rules to create a final rule-based churn prediction\n",
    "data['rule_based_churn'] = data[['rule_no_transactions_12_months', 'rule_low_transaction_amount', 'rule_single_transaction_type']].max(axis=1)\n",
    "\n",
    "# Display the first few rows to verify the rule-based churn predictions\n",
    "data[['customer_id', 'rule_no_transactions_12_months', 'rule_low_transaction_amount', 'rule_single_transaction_type', 'rule_based_churn']].head()\n",
    "\n",
    "# Plot the distribution of the rule-based churn variable\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(data['rule_based_churn'])\n",
    "plt.title('Distribution of Rule-Based Churn')\n",
    "plt.xticks([0, 1], ['Not Churned', 'Churned'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Orchestration\n",
    "Create a system to orchestrate between rule-based predictions and model predictions based on outlier scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Orchestration\n",
    "\n",
    "# Define a function to orchestrate between rule-based predictions and model predictions based on outlier scores\n",
    "def orchestrate_predictions(row, model, threshold=-0.5):\n",
    "    if row['outlier_score'] >= threshold:\n",
    "        return row['rule_based_churn']\n",
    "    else:\n",
    "        return model.predict(pd.DataFrame([row[features]]))[0]\n",
    "\n",
    "# Apply the orchestration function to the dataset\n",
    "data['final_churn_prediction'] = data.apply(orchestrate_predictions, axis=1, model=best_model)\n",
    "\n",
    "# Display the first few rows to verify the final churn predictions\n",
    "data[['customer_id', 'rule_based_churn', 'outlier_score', 'final_churn_prediction']].head()\n",
    "\n",
    "# Plot the distribution of the final churn predictions\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(data['final_churn_prediction'])\n",
    "plt.title('Distribution of Final Churn Predictions')\n",
    "plt.xticks([0, 1], ['Not Churned', 'Churned'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "Evaluate the complete system using appropriate metrics and analyze feature importance for explainability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "\n",
    "# Import necessary libraries for evaluation\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "\n",
    "# Evaluate the final churn predictions\n",
    "y_true = data[target]\n",
    "y_pred_final = data['final_churn_prediction']\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_true, y_pred_final)\n",
    "precision = precision_score(y_true, y_pred_final)\n",
    "recall = recall_score(y_true, y_pred_final)\n",
    "f1 = f1_score(y_true, y_pred_final)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Calculate ROC curve and AUC\n",
    "fpr, tpr, _ = roc_curve(y_true, y_pred_final)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Analyze feature importance for explainability\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(features, best_model.feature_importances_)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importance of the XGBoost Model')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
