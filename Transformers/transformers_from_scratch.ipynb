{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 126] The specified module could not be found. Error loading \"C:\\Users\\My-PC\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\lib\\fbgemm.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01membed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_embedding\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\embed\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01membed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_infer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BatchedInference\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetadata\u001b[39;00m\n\u001b[0;32m      4\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfinity_emb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\embed\\_infer.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconcurrent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfutures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Future\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Collection, Literal, Union\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minfinity_emb\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EngineArgs, SyncEngineArray  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minfinity_emb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minfinity_server\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoPadding\n\u001b[0;32m      7\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatchedInference\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\infinity_emb\\__init__.py:20\u001b[0m\n\u001b[0;32m     16\u001b[0m huggingface_hub\u001b[38;5;241m.\u001b[39mconstants\u001b[38;5;241m.\u001b[39mHF_HUB_DISABLE_PROGRESS_BARS \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minfinity_emb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01margs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EngineArgs  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minfinity_emb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AsyncEmbeddingEngine, AsyncEngineArray  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minfinity_emb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MANAGER  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# reexports\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\infinity_emb\\engine.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minfinity_emb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01margs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EngineArgs\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# prometheus\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minfinity_emb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      8\u001b[0m     BatchHandler,\n\u001b[0;32m      9\u001b[0m     select_model,\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minfinity_emb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlog_handler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logger\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minfinity_emb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprimitives\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     13\u001b[0m     ClassifyReturnType,\n\u001b[0;32m     14\u001b[0m     EmbeddingReturnType,\n\u001b[0;32m     15\u001b[0m     ModelCapabilites,\n\u001b[0;32m     16\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\infinity_emb\\inference\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minfinity_emb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatch_handler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BatchHandler\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minfinity_emb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mselect_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m select_model\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minfinity_emb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprimitives\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      4\u001b[0m     Device,\n\u001b[0;32m      5\u001b[0m     EmbeddingInner,\n\u001b[0;32m      6\u001b[0m     EmbeddingReturnType,\n\u001b[0;32m      7\u001b[0m     PrioritizedQueueItem,\n\u001b[0;32m      8\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\infinity_emb\\inference\\batch_handler.py:29\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minfinity_emb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlog_handler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logger\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minfinity_emb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprimitives\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m     AbstractSingle,\n\u001b[0;32m     18\u001b[0m     ClassifyReturnType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m     get_inner_item,\n\u001b[0;32m     28\u001b[0m )\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minfinity_emb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabstract\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseTransformer\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minfinity_emb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_lengths_with_tokenize\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minfinity_emb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m resolve_images\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\infinity_emb\\transformer\\abstract.py:20\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minfinity_emb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_optional_imports\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CHECK_PIL\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minfinity_emb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprimitives\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      8\u001b[0m     EmbeddingDtype,\n\u001b[0;32m      9\u001b[0m     EmbeddingInner,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     ReRankSingle,\n\u001b[0;32m     19\u001b[0m )\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minfinity_emb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m quant_embedding_decorator\n\u001b[0;32m     22\u001b[0m INPUT_FEATURE \u001b[38;5;241m=\u001b[39m Any\n\u001b[0;32m     23\u001b[0m OUT_FEATURES \u001b[38;5;241m=\u001b[39m Any\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\infinity_emb\\transformer\\quantization\\interface.py:12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minfinity_emb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlog_handler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logger\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minfinity_emb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprimitives\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Device, Dtype, EmbeddingDtype\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minfinity_emb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquant\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m quantize\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minfinity_emb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabstract\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseEmbedder\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\infinity_emb\\transformer\\quantization\\quant.py:19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minfinity_emb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_optional_imports\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CHECK_TORCH\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minfinity_emb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlog_handler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logger\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mCHECK_TORCH\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_available\u001b[49m:\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\functools.py:981\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, instance, owner)\u001b[0m\n\u001b[0;32m    979\u001b[0m val \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname, _NOT_FOUND)\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[1;32m--> 981\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    982\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    983\u001b[0m         cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname] \u001b[38;5;241m=\u001b[39m val\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\infinity_emb\\_optional_imports.py:30\u001b[0m, in \u001b[0;36mOptionalImports.is_available\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(lib)):\n\u001b[0;32m     29\u001b[0m         module \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(lib[: i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m---> 30\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_spec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     31\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mfind_spec(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlib) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\importlib\\util.py:94\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     92\u001b[0m parent_name \u001b[38;5;241m=\u001b[39m fullname\u001b[38;5;241m.\u001b[39mrpartition(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parent_name:\n\u001b[1;32m---> 94\u001b[0m     parent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43m__import__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparent_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfromlist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m__path__\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     96\u001b[0m         parent_path \u001b[38;5;241m=\u001b[39m parent\u001b[38;5;241m.\u001b[39m__path__\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\__init__.py:148\u001b[0m\n\u001b[0;32m    146\u001b[0m                 err \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mWinError(ctypes\u001b[38;5;241m.\u001b[39mget_last_error())\n\u001b[0;32m    147\u001b[0m                 err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 148\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    150\u001b[0m     kernel32\u001b[38;5;241m.\u001b[39mSetErrorMode(prev_error_mode)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_preload_cuda_deps\u001b[39m(lib_folder, lib_name):\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 126] The specified module could not be found. Error loading \"C:\\Users\\My-PC\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\lib\\fbgemm.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from embed import get_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(scores):\n",
    "    \"\"\"\n",
    "    Apply softmax to normalize attention scores\n",
    "\n",
    "    Args:\n",
    "    scores(np.ndarray): Attention scores\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: Normalized attention scores\n",
    "    \"\"\"\n",
    "    exp_scores = np.exp(scores, axis = 1, keepdims = True) # numerical stability and normalizes across each row (ie across all key vectors for each query vector)\n",
    "    # keepdims = True to keep the dimension of the original array so that we can broadcast it\n",
    "    return exp_scores / np.sum(exp_scores)\n",
    "\n",
    "class SelfAttention: # It is the layer that computes the attention scores between the query and key vectors and then applies the attention scores to the value vectors to get the output\n",
    "    \"\"\"\n",
    "    Self Attention mechanism to compute attention scores\n",
    "    Attributes:\n",
    "    embed_dim(int): Embedding dimension\n",
    "    W_q(np.ndarray): Weight matrix for query # query is the vector that we want to compare to all the keys\n",
    "    W_k(np.ndarray): Weight matrix for key # key is the vector that we want to compare to the query\n",
    "    W_v(np.ndarray): Weight matrix for value # value is the vector that we want to output\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim):\n",
    "        \"\"\"\n",
    "        Initialize the SelfAttention mechanism\n",
    "        Args:\n",
    "            embed_dim(int): Embedding dimension\n",
    "        \"\"\"\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Initialize the weight matrices with small random numbers from a normal distribution\n",
    "        self.W_q = np.random.randn(embedding_dim, embedding_dim) * np.sqrt(1. / embedding_dim)\n",
    "        self.W_k = np.random.randn(embedding_dim, embedding_dim) * np.sqrt(1. / embedding_dim)\n",
    "        self.W_v = np.random.randn(embedding_dim, embedding_dim) * np.sqrt(1. / embedding_dim)\n",
    "\n",
    "    def forward(self, emdeddings, mask = None):\n",
    "        \"\"\"\n",
    "        Forward pass through the self attention mechanism to compute attention scores\n",
    "        Args:\n",
    "            embeddings(np.ndarray): Input embeddings\n",
    "            mask(np.ndarray, optional): Mask to be applied to the attention scores to avoid attending to certain elements\n",
    "        Returns:\n",
    "            np.ndarray: Attention scores (output after applying attention scores to the values)\n",
    "        \"\"\"\n",
    "        query = np.dot(emdeddings, self.W_q)\n",
    "        key = np.dot(emdeddings, self.W_k)\n",
    "        values = np.dot(emdeddings, self.W_v)\n",
    "\n",
    "        # Compute the attention scores\n",
    "        attention_scores = np.calculate_attention_scores(query, key)\n",
    "\n",
    "        # Masking\n",
    "        if mask is not None:\n",
    "            attention_scores = np.where(mask==0, -1e9, attention_scores) # if mask is 0, then set the attention score to -1e9\n",
    "            # if mask is 1, then keep the attention score as it is\n",
    "\n",
    "        # Apply softmax to normalize the attention scores\n",
    "        attention_weights = softmax(attention_scores)\n",
    "        output = self.values_weighted_sum(values, attention_weights)\n",
    "        return output\n",
    "    \n",
    "    def calculate_attention_score(self, query, key):\n",
    "        \"\"\"\n",
    "        Calculate attention scores based on the query and key matrices\n",
    "        Args:\n",
    "            query(np.ndarray): Query matrix\n",
    "            key(np.ndarray): Key matrix\n",
    "        Returns:\n",
    "            np.ndarray: Attention scores\n",
    "        \"\"\"\n",
    "        d_k = query.shape[-1] # scaling factor to ensure that not too large values are fed to the softmax function as it would result in numerical instability \n",
    "                            # (would push softmax into regions where the gradients are very small)\n",
    "        dot = np.dot(query, key.T) # key.T is the transpose of the key matrix\n",
    "                                # flipping the matrix over its diagonal,such that the row and column indices are switched\n",
    "        return dot / np.sqrt(d_k) # scale by the square root of the key dimension\n",
    "    \n",
    "    def values_weighted_sum(self, weights, values):\n",
    "        \"\"\"\n",
    "        Weighted sum of values based on the attention weights\n",
    "        Args:\n",
    "            weights(np.ndarray): Attention weights\n",
    "            values(np.ndarray): Values to be weighted\n",
    "        Returns:\n",
    "            np.ndarray: Weighted sum of values\n",
    "        \"\"\"\n",
    "        return np.dot(weights, values)\n",
    "    \n",
    "class MultiHeadAttention: # It is the layer that splits the input embeddings into multiple heads, applies the self attention mechanism to each head, concatenates the heads, and applies the output weight matrix to get the output\n",
    "    \"\"\"\n",
    "    Multi-head attention mechanism consisting of multiple self attention heads\n",
    "    Attributes:\n",
    "    head_dim(int): Dimension of each attention head\n",
    "    attention_heads(list): list of self attention heads\n",
    "    W_o(np.ndarray): Weight matrix for output\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim, num_heads):\n",
    "        \"\"\"\n",
    "        Initialize the MultiHeadAttention mechanism\n",
    "        Args:\n",
    "            embed_dim(int): Embedding dimension\n",
    "            num_heads(int): Number of attention heads\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If the embedding dimension is not divisible by the number of heads\n",
    "        \"\"\"\n",
    "        # embedding_dim must be divisible by num_heads\n",
    "        # otherwise, the context window will be different for each head ie inconsistent context window\n",
    "        if embedding_dim % num_heads != 0:\n",
    "            raise ValueError(\"Embedding dimension must be divisible by the number of heads\")\n",
    "        \n",
    "        # compute the dimension of each head\n",
    "        self.head_dim = embedding_dim // num_heads\n",
    "\n",
    "        # Initialize the attention heads (instances of the SelfAttention class)\n",
    "        self.attention_heads = [SelfAttention(self.head_dim) for _ in range(num_heads)]\n",
    "\n",
    "        # Initialize the output weight matrix\n",
    "        self.W_o = np.random.randn(embedding_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        \"\"\"\n",
    "        Forward pass through the multi-head attention mechanism to compute attention scores\n",
    "        Args:\n",
    "            embeddings(np.ndarray): Input embeddings\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Attention scores (output after applying attention scores to the values)\n",
    "        \"\"\"\n",
    "        # Split the embeddings into multiple heads\n",
    "        sequence_length, embedding_dim = embeddings.shape\n",
    "        split_embeddings = np.reshape(embeddings, (sequence_length, -1, self.head_dim))\n",
    "\n",
    "        head_outputs = []\n",
    "        for i, head in enumerate(self.attention_heads):\n",
    "            head_output = head.forward[:, i, :]\n",
    "            head_outputs.append(head_output)\n",
    "\n",
    "        # Concatenate the heads\n",
    "        concatenated_output = np.concatenate(head_outputs, axis = -1)\n",
    "\n",
    "        # Apply the output weight matrix\n",
    "        output = self.linear_transform(concatenated_output, self.W_o)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def linear_transformation(self, concatenated_output, weight_matrix):\n",
    "        \"\"\"\n",
    "        Apply linear transformation to the concatenated output\n",
    "        Args:\n",
    "            concatenated_output(np.ndarray): Concatenated output of the attention heads\n",
    "            weight_matrix(np.ndarray): Weight matrix for output\n",
    "        Returns:\n",
    "            np.ndarray: Output after applying the linear transformation\n",
    "        \"\"\"\n",
    "        return np.dot(concatenated_output, weight_matrix)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
